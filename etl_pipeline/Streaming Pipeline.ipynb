{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e21a3a7-e795-4f43-8bf5-f561324bf24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup Widgets for Parameterization\n",
    "dbutils.widgets.text(\"source_path\", \"/Volumes/workspace/e-commerce_data/csv_files\", \"1. Source Data Path\")\n",
    "dbutils.widgets.text(\"target_database\", \"streaming_db\", \"2. Target Streaming Database\")\n",
    "\n",
    "# 2. Get Parameters\n",
    "source_path = dbutils.widgets.get(\"source_path\")\n",
    "target_database = dbutils.widgets.get(\"target_database\")\n",
    "\n",
    "checkpoint_base_path = \"/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline\"\n",
    "\n",
    "# 3. Prepare Environment\n",
    "dbutils.fs.mkdirs(checkpoint_base_path)\n",
    "\n",
    "print(\"--- Configuration ---\")\n",
    "print(f\"Source Path:          {source_path}\")\n",
    "print(f\"Target Database:      {target_database}\")\n",
    "print(f\"Checkpoint Base Path: {checkpoint_base_path}\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 4. Import Pipeline Functions\n",
    "import sys\n",
    "sys.path.append('/Workspace/Repos/3hmedgomaa2001@gmail.com/Ecommerce-Databricks-Solution')\n",
    "\n",
    "from etl_pipeline.transformers import create_silver_layer, create_gold_layers\n",
    "from etl_pipeline.writer import write_delta_table\n",
    "\n",
    "# 5. Define the Micro-Batch Processing Function\n",
    "def process_micro_batch(micro_batch_df, batch_id):\n",
    "    print(f\"\\n--- Processing Micro-Batch ID: {batch_id} ---\")\n",
    "\n",
    "    try:\n",
    "        # Load static bronze tables\n",
    "        properties_df = spark.table(f\"{target_database}.properties_bronze\")\n",
    "        category_df = spark.table(f\"{target_database}.categories_bronze\")\n",
    "\n",
    "        # Combine micro-batch with other bronze tables\n",
    "        bronze_dfs_for_batch = {\n",
    "            \"events\": micro_batch_df,\n",
    "            \"properties\": properties_df,\n",
    "            \"categories\": category_df\n",
    "        }\n",
    "\n",
    "        # --- Silver Transformation ---\n",
    "        silver_micro_batch_df = create_silver_layer(bronze_dfs_for_batch, skip_quality_check=True)\n",
    "\n",
    "        # Write Silver micro-batch\n",
    "        silver_table_name = f\"{target_database}.events_enriched_silver_stream\"\n",
    "        silver_micro_batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_name)\n",
    "        print(f\"Appended micro-batch {batch_id} to {silver_table_name}\")\n",
    "\n",
    "        # --- Gold Transformation ---\n",
    "        gold_dfs = create_gold_layers(silver_micro_batch_df)\n",
    "\n",
    "        # Write each Gold table\n",
    "        for name, df in gold_dfs.items():\n",
    "            table_name = f\"{target_database}.{name}_stream\"\n",
    "            df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "            print(f\"Updated Gold table: {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing micro-batch {batch_id}: {e}\")\n",
    "\n",
    "# 6. Define the Streaming Source\n",
    "events_stream_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base_path}/schema/events\") \\\n",
    "    .load(source_path)\n",
    "\n",
    "# 7. Start the Stream\n",
    "print(f\"\\nDEBUG: Initializing stream with checkpointLocation: '{checkpoint_base_path}'\") \n",
    "\n",
    "stream_query = events_stream_df.writeStream \\\n",
    "    .foreachBatch(process_micro_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_base_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"\\n--- Streaming query started. Waiting for completion... ---\")\n",
    "stream_query.awaitTermination()\n",
    "print(\"--- Streaming query has completed. ---\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming Pipeline",
   "widgets": {
    "checkpoint_path": {
     "currentValue": "/tmp/databricks/checkpoints/ecommerce_pipeline",
     "nuid": "89aab075-32c8-4a7e-84b2-d4bea980f422",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline",
      "label": "3. Checkpoint Path",
      "name": "checkpoint_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline",
      "label": "3. Checkpoint Path",
      "name": "checkpoint_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_path": {
     "currentValue": "/Volumes/workspace/e-commerce_data/csv_files",
     "nuid": "ec1a2368-f3dd-4b78-8d70-a2e283ce0906",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files",
      "label": "1. Source Data Path",
      "name": "source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files",
      "label": "1. Source Data Path",
      "name": "source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_database": {
     "currentValue": "E_ComStreamingDB",
     "nuid": "4e25b237-c0f9-4ec7-bb9d-96100317698c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "streaming_db",
      "label": "2. Target Streaming Database",
      "name": "target_database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "streaming_db",
      "label": "2. Target Streaming Database",
      "name": "target_database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
