{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92000148-2096-4b3c-b6c6-0cc5447fcb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup Widgets for Parameterization\n",
    "dbutils.widgets.text(\"source_path\", \"/Volumes/workspace/e-commerce_data/csv_files\", \"1. Source Data Path\")\n",
    "dbutils.widgets.text(\"target_database\", \"streaming_db\", \"2. Target Streaming Database\")\n",
    "dbutils.widgets.text(\"checkpoint_path\", \"/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline\", \"3. Checkpoint Path\")\n",
    "\n",
    "# 2. Get Parameters\n",
    "source_path = dbutils.widgets.get(\"source_path\")\n",
    "target_database = dbutils.widgets.get(\"target_database\")\n",
    "checkpoint_base_path = dbutils.widgets.get(\"checkpoint_path\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {target_database}\")\n",
    "\n",
    "# --- THIS IS THE CORRECTED LINE ---\n",
    "# It now uses the variable from the widget above.\n",
    "dbutils.fs.rm(checkpoint_base_path, recurse=True)\n",
    "\n",
    "print(f\"Source Path: {source_path}\")\n",
    "print(f\"Target Database: {target_database}\")\n",
    "print(f\"Checkpoint Path: {checkpoint_base_path}\")\n",
    "\n",
    "# 3. Import Pipeline Functions\n",
    "import sys\n",
    "sys.path.append('/Workspace/Repos/3hmedgomaa2001@gmail.com/Ecommerce-Databricks-Solution')\n",
    "\n",
    "from etl_pipeline.transformers import create_silver_layer\n",
    "from etl_pipeline.writer import write_delta_table\n",
    "\n",
    "\n",
    "def process_micro_batch(micro_batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    This function is applied to each micro-batch of data that the stream processes.\n",
    "    It reuses our existing batch transformation logic.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing Micro-Batch ID: {batch_id} ---\")\n",
    "    \n",
    "\n",
    "    properties_df = spark.table(\"default.properties_bronze\") \n",
    "    category_df = spark.table(\"default.categories_bronze\")\n",
    "    \n",
    "    bronze_dfs_for_batch = {\n",
    "        \"events\": micro_batch_df,\n",
    "        \"properties\": properties_df,\n",
    "        \"categories\": category_df\n",
    "    }\n",
    "    \n",
    "    # 1. Create the Silver layer for this micro-batch\n",
    "    silver_micro_batch_df = create_silver_layer(bronze_dfs_for_batch)\n",
    "    \n",
    "    # 2. Write the resulting silver data to a streaming target table\n",
    "    silver_table_name = f\"{target_database}.events_enriched_silver_stream\"\n",
    "    silver_micro_batch_df.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_name)\n",
    "    print(f\"Appended micro-batch to {silver_table_name}\")\n",
    "    \n",
    "\n",
    "events_stream_df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{source_path}/events.csv\")\n",
    "\n",
    "stream_query = events_stream_df.writeStream \\\n",
    "    .foreachBatch(process_micro_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_base_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming Pipeline",
   "widgets": {
    "checkpoint_path": {
     "currentValue": "/tmp/databricks/checkpoints/ecommerce_pipeline",
     "nuid": "89aab075-32c8-4a7e-84b2-d4bea980f422",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline",
      "label": "3. Checkpoint Path",
      "name": "checkpoint_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files/_checkpoints/ecommerce_pipeline",
      "label": "3. Checkpoint Path",
      "name": "checkpoint_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_path": {
     "currentValue": "/Volumes/workspace/e-commerce_data/csv_files",
     "nuid": "ec1a2368-f3dd-4b78-8d70-a2e283ce0906",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files",
      "label": "1. Source Data Path",
      "name": "source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/e-commerce_data/csv_files",
      "label": "1. Source Data Path",
      "name": "source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_database": {
     "currentValue": "streaming_db",
     "nuid": "4e25b237-c0f9-4ec7-bb9d-96100317698c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "streaming_db",
      "label": "2. Target Streaming Database",
      "name": "target_database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "streaming_db",
      "label": "2. Target Streaming Database",
      "name": "target_database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
